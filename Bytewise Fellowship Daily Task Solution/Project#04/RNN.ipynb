{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PART 2 OF PROJECT 04**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Implementing a Basic RNN Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I build a basic Recurrent Neural Network (RNN) to perform sentiment analysis on the IMDB dataset. The IMDB dataset consists of 50,000 movie reviews labeled as either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "vocabulary_size = 20000  # Maximum number of unique words to consider in the dataset\n",
    "max_review_length = 1000  # Maximum length of each review (in number of words)\n",
    "batch_size = 32  # Number of samples processed before the model is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset, limiting the vocabulary size\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocabulary_size)\n",
    "\n",
    "# Pad sequences to ensure consistent review lengths\n",
    "train_data = sequence.pad_sequences(train_data, maxlen=max_review_length)\n",
    "test_data = sequence.pad_sequences(test_data, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Building the Basic RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building RNN model\n",
    "basic_rnn_model = Sequential()\n",
    "basic_rnn_model.add(Embedding(vocabulary_size, 32, input_length=max_review_length))\n",
    "basic_rnn_model.add(SimpleRNN(32, return_sequences=False))\n",
    "basic_rnn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Compiling and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 203ms/step - accuracy: 0.5512 - loss: 0.6773 - val_accuracy: 0.7720 - val_loss: 0.4857\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 193ms/step - accuracy: 0.8140 - loss: 0.4218 - val_accuracy: 0.8430 - val_loss: 0.3767\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 198ms/step - accuracy: 0.8743 - loss: 0.3025 - val_accuracy: 0.8414 - val_loss: 0.4188\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 201ms/step - accuracy: 0.9107 - loss: 0.2237 - val_accuracy: 0.8416 - val_loss: 0.4099\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 201ms/step - accuracy: 0.9369 - loss: 0.1699 - val_accuracy: 0.8346 - val_loss: 0.4778\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 189ms/step - accuracy: 0.9596 - loss: 0.1105 - val_accuracy: 0.8402 - val_loss: 0.5403\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 199ms/step - accuracy: 0.9769 - loss: 0.0693 - val_accuracy: 0.8236 - val_loss: 0.6187\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 201ms/step - accuracy: 0.9805 - loss: 0.0585 - val_accuracy: 0.7868 - val_loss: 0.7533\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 217ms/step - accuracy: 0.9899 - loss: 0.0359 - val_accuracy: 0.8010 - val_loss: 0.7926\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 188ms/step - accuracy: 0.9875 - loss: 0.0379 - val_accuracy: 0.7992 - val_loss: 0.8194\n"
     ]
    }
   ],
   "source": [
    "basic_rnn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_basic_rnn = basic_rnn_model.fit(train_data, train_labels, epochs=10, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.7932 - loss: 0.8664\n",
      "Basic RNN Test Accuracy: 0.794160008430481\n"
     ]
    }
   ],
   "source": [
    "test_loss_basic_rnn, test_accuracy_basic_rnn = basic_rnn_model.evaluate(test_data, test_labels)\n",
    "print(f'Basic RNN Test Accuracy: {test_accuracy_basic_rnn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Stacking RNN Layers and Bi-directional RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explores more advanced RNN architectures by stacking multiple RNN layers and using bidirectional RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2a. Stacked RNN Model**\n",
    "\n",
    "#### Step 1: Building the Stacked RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rnn_model = Sequential()\n",
    "stacked_rnn_model.add(Embedding(vocabulary_size, 32, input_length=max_review_length))\n",
    "stacked_rnn_model.add(SimpleRNN(32, return_sequences=True))  # First RNN layer\n",
    "stacked_rnn_model.add(SimpleRNN(32, return_sequences=False))  # Second RNN layer\n",
    "stacked_rnn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compiling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 353ms/step - accuracy: 0.6321 - loss: 0.6176 - val_accuracy: 0.8274 - val_loss: 0.3952\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 351ms/step - accuracy: 0.8364 - loss: 0.3797 - val_accuracy: 0.8574 - val_loss: 0.3546\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 366ms/step - accuracy: 0.8862 - loss: 0.2862 - val_accuracy: 0.8318 - val_loss: 0.4095\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 383ms/step - accuracy: 0.9239 - loss: 0.2052 - val_accuracy: 0.8464 - val_loss: 0.4430\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 381ms/step - accuracy: 0.9469 - loss: 0.1440 - val_accuracy: 0.8438 - val_loss: 0.5258\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 403ms/step - accuracy: 0.9682 - loss: 0.0868 - val_accuracy: 0.8284 - val_loss: 0.7459\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 403ms/step - accuracy: 0.9803 - loss: 0.0588 - val_accuracy: 0.7552 - val_loss: 1.0316\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 376ms/step - accuracy: 0.9857 - loss: 0.0446 - val_accuracy: 0.7924 - val_loss: 0.8662\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 433ms/step - accuracy: 0.9931 - loss: 0.0240 - val_accuracy: 0.7960 - val_loss: 0.9531\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 423ms/step - accuracy: 0.9930 - loss: 0.0248 - val_accuracy: 0.8086 - val_loss: 0.9407\n"
     ]
    }
   ],
   "source": [
    "# Compiling model\n",
    "stacked_rnn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history_stacked_rnn = stacked_rnn_model.fit(train_data, train_labels, epochs=10, batch_size=batch_size, validation_split=0.2)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 88ms/step - accuracy: 0.7987 - loss: 0.9941\n",
      "Stacked RNN Test Accuracy: 0.8014400005340576\n"
     ]
    }
   ],
   "source": [
    "test_loss_stacked_rnn, test_accuracy_stacked_rnn = stacked_rnn_model.evaluate(test_data, test_labels)\n",
    "print(f'Stacked RNN Test Accuracy: {test_accuracy_stacked_rnn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2b. Bi-directional RNN Model**\n",
    "\n",
    "#### Step 1: Building the Bi-directional RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "bidirectional_rnn_model = Sequential()\n",
    "bidirectional_rnn_model.add(Embedding(vocabulary_size, 32, input_length=max_review_length))\n",
    "bidirectional_rnn_model.add(Bidirectional(SimpleRNN(32)))\n",
    "bidirectional_rnn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compiling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 284ms/step - accuracy: 0.6011 - loss: 0.6335 - val_accuracy: 0.8346 - val_loss: 0.3951\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 290ms/step - accuracy: 0.8109 - loss: 0.4344 - val_accuracy: 0.8506 - val_loss: 0.3646\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 439ms/step - accuracy: 0.8656 - loss: 0.3343 - val_accuracy: 0.8650 - val_loss: 0.3423\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 510ms/step - accuracy: 0.8845 - loss: 0.2887 - val_accuracy: 0.8630 - val_loss: 0.3548\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 436ms/step - accuracy: 0.9032 - loss: 0.2516 - val_accuracy: 0.8474 - val_loss: 0.3837\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 421ms/step - accuracy: 0.9181 - loss: 0.2120 - val_accuracy: 0.8648 - val_loss: 0.3904\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 425ms/step - accuracy: 0.9382 - loss: 0.1661 - val_accuracy: 0.8196 - val_loss: 0.4342\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 406ms/step - accuracy: 0.9382 - loss: 0.1680 - val_accuracy: 0.8158 - val_loss: 0.5255\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 405ms/step - accuracy: 0.9556 - loss: 0.1213 - val_accuracy: 0.8488 - val_loss: 0.5319\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 407ms/step - accuracy: 0.9677 - loss: 0.0919 - val_accuracy: 0.8368 - val_loss: 0.6418\n"
     ]
    }
   ],
   "source": [
    "bidirectional_rnn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_bidirectional_rnn = bidirectional_rnn_model.fit(train_data, train_labels, epochs=10, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 75ms/step - accuracy: 0.8346 - loss: 0.6333\n",
      "Bidirectional RNN Test Accuracy: 0.8339200019836426\n"
     ]
    }
   ],
   "source": [
    "test_loss_bidirectional_rnn, test_accuracy_bidirectional_rnn = bidirectional_rnn_model.evaluate(test_data, test_labels)\n",
    "print(f'Bidirectional RNN Test Accuracy: {test_accuracy_bidirectional_rnn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Exploring Hybrid Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I explore combining Convolutional Neural Networks (CNNs) with RNNs to create hybrid architectures. These models leverage CNNs to capture local patterns and RNNs to capture sequential dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Building the Hybrid Model (RNN + CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "hybrid_cnn_rnn_model = Sequential()\n",
    "hybrid_cnn_rnn_model.add(Embedding(vocabulary_size, 32, input_length=max_review_length))\n",
    "hybrid_cnn_rnn_model.add(Conv1D(32, 7, activation='relu'))\n",
    "hybrid_cnn_rnn_model.add(MaxPooling1D(5))\n",
    "hybrid_cnn_rnn_model.add(Conv1D(32, 7, activation='relu'))\n",
    "hybrid_cnn_rnn_model.add(GlobalMaxPooling1D())\n",
    "hybrid_cnn_rnn_model.add(Dense(32, activation='relu'))\n",
    "hybrid_cnn_rnn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compiling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 47ms/step - accuracy: 0.6122 - loss: 0.6138 - val_accuracy: 0.8608 - val_loss: 0.3309\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.8679 - loss: 0.3136 - val_accuracy: 0.8754 - val_loss: 0.3112\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - accuracy: 0.9140 - loss: 0.2229 - val_accuracy: 0.8746 - val_loss: 0.3213\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - accuracy: 0.9391 - loss: 0.1633 - val_accuracy: 0.8776 - val_loss: 0.3273\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - accuracy: 0.9604 - loss: 0.1132 - val_accuracy: 0.8782 - val_loss: 0.3622\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 43ms/step - accuracy: 0.9783 - loss: 0.0706 - val_accuracy: 0.8752 - val_loss: 0.4380\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - accuracy: 0.9886 - loss: 0.0396 - val_accuracy: 0.8712 - val_loss: 0.5234\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - accuracy: 0.9954 - loss: 0.0197 - val_accuracy: 0.8702 - val_loss: 0.6855\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - accuracy: 0.9958 - loss: 0.0142 - val_accuracy: 0.8642 - val_loss: 0.8203\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 46ms/step - accuracy: 0.9982 - loss: 0.0055 - val_accuracy: 0.8440 - val_loss: 1.2113\n"
     ]
    }
   ],
   "source": [
    "hybrid_cnn_rnn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_hybrid_cnn_rnn = hybrid_cnn_rnn_model.fit(train_data, train_labels, epochs=10, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8360 - loss: 1.2316\n",
      "Hybrid CNN+RNN Test Accuracy: 0.8390799760818481\n"
     ]
    }
   ],
   "source": [
    "test_loss_hybrid_cnn_rnn, test_accuracy_hybrid_cnn_rnn = hybrid_cnn_rnn_model.evaluate(test_data, test_labels)\n",
    "print(f'Hybrid CNN+RNN Test Accuracy: {test_accuracy_hybrid_cnn_rnn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this notebook, I explored different architectures for sentiment analysis using the IMDB dataset. Me started with a basic RNN, then explored stacked RNNs, bidirectional RNNs, and hybrid CNN-RNN models. Each model offers unique advantages and challenges, and the choice of architecture should depend on the specific use case.\n",
    "\n",
    "**Basic RNN:** A good starting point for sequence modeling.\n",
    "\n",
    "**Stacked RNN:** Improves model capacity by adding more layers.\n",
    "\n",
    "**Bidirectional RNN:** Provides context from both directions, improving performance on certain tasks.\n",
    "\n",
    "**Hybrid CNN-RNN:** Combines the strengths of CNNs and RNNs for sequence modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
